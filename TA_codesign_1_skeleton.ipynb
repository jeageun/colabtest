{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TA_codesign_1_skeleton",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOhuKyYGvCtm8nTpLNpKkTs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeageun/colabtest/blob/main/TA_codesign_1_skeleton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import easydict"
      ],
      "metadata": {
        "id": "-B_cFrbYhqqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Skeleton code for CNN"
      ],
      "metadata": {
        "id": "kBLEylyZiVM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.features = nn.Sequential()\n",
        "        self.features.add_module(\"conv1\", nn.Conv2d(1, 4, kernel_size=3, stride=1, padding=1))\n",
        "        self.features.add_module(\"pool1\", nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.features.add_module(\"conv2\", nn.Conv2d(4, 16, kernel_size=3, stride=1, padding=1))\n",
        "        self.features.add_module(\"pool2\", nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.lin1 = nn.Linear(7 * 7 * 16, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.lin1(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "kJrjg_a22xg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mx8mgJ7K2t-R",
        "outputId": "9f649946-8e13-4a26-fac0-d05cc0c006a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [ 1/ 10], Step: [ 100/ 600], Loss: 2.1713\n",
            "Epoch: [ 1/ 10], Step: [ 200/ 600], Loss: 2.0883\n",
            "Epoch: [ 1/ 10], Step: [ 300/ 600], Loss: 1.9803\n",
            "Epoch: [ 1/ 10], Step: [ 400/ 600], Loss: 1.8820\n",
            "Epoch: [ 1/ 10], Step: [ 500/ 600], Loss: 1.8493\n",
            "Epoch: [ 1/ 10], Step: [ 600/ 600], Loss: 1.7748\n",
            "Epoch: [ 2/ 10], Step: [ 100/ 600], Loss: 1.6770\n",
            "Epoch: [ 2/ 10], Step: [ 200/ 600], Loss: 1.6850\n",
            "Epoch: [ 2/ 10], Step: [ 300/ 600], Loss: 1.5919\n",
            "Epoch: [ 2/ 10], Step: [ 400/ 600], Loss: 1.5188\n",
            "Epoch: [ 2/ 10], Step: [ 500/ 600], Loss: 1.5097\n",
            "Epoch: [ 2/ 10], Step: [ 600/ 600], Loss: 1.3816\n",
            "Epoch: [ 3/ 10], Step: [ 100/ 600], Loss: 1.4018\n",
            "Epoch: [ 3/ 10], Step: [ 200/ 600], Loss: 1.3814\n",
            "Epoch: [ 3/ 10], Step: [ 300/ 600], Loss: 1.3112\n",
            "Epoch: [ 3/ 10], Step: [ 400/ 600], Loss: 1.2709\n",
            "Epoch: [ 3/ 10], Step: [ 500/ 600], Loss: 1.2654\n",
            "Epoch: [ 3/ 10], Step: [ 600/ 600], Loss: 1.1818\n",
            "Epoch: [ 4/ 10], Step: [ 100/ 600], Loss: 1.1968\n",
            "Epoch: [ 4/ 10], Step: [ 200/ 600], Loss: 1.2537\n",
            "Epoch: [ 4/ 10], Step: [ 300/ 600], Loss: 1.1686\n",
            "Epoch: [ 4/ 10], Step: [ 400/ 600], Loss: 1.2038\n",
            "Epoch: [ 4/ 10], Step: [ 500/ 600], Loss: 1.1799\n",
            "Epoch: [ 4/ 10], Step: [ 600/ 600], Loss: 1.0867\n",
            "Epoch: [ 5/ 10], Step: [ 100/ 600], Loss: 1.1583\n",
            "Epoch: [ 5/ 10], Step: [ 200/ 600], Loss: 1.1009\n",
            "Epoch: [ 5/ 10], Step: [ 300/ 600], Loss: 1.0636\n",
            "Epoch: [ 5/ 10], Step: [ 400/ 600], Loss: 0.9630\n",
            "Epoch: [ 5/ 10], Step: [ 500/ 600], Loss: 1.0596\n",
            "Epoch: [ 5/ 10], Step: [ 600/ 600], Loss: 1.0200\n",
            "Epoch: [ 6/ 10], Step: [ 100/ 600], Loss: 0.9587\n",
            "Epoch: [ 6/ 10], Step: [ 200/ 600], Loss: 0.9904\n",
            "Epoch: [ 6/ 10], Step: [ 300/ 600], Loss: 1.0227\n",
            "Epoch: [ 6/ 10], Step: [ 400/ 600], Loss: 0.8823\n",
            "Epoch: [ 6/ 10], Step: [ 500/ 600], Loss: 1.0196\n",
            "Epoch: [ 6/ 10], Step: [ 600/ 600], Loss: 0.9277\n",
            "Epoch: [ 7/ 10], Step: [ 100/ 600], Loss: 0.9160\n",
            "Epoch: [ 7/ 10], Step: [ 200/ 600], Loss: 0.9210\n",
            "Epoch: [ 7/ 10], Step: [ 300/ 600], Loss: 0.7798\n",
            "Epoch: [ 7/ 10], Step: [ 400/ 600], Loss: 0.9281\n",
            "Epoch: [ 7/ 10], Step: [ 500/ 600], Loss: 0.8014\n",
            "Epoch: [ 7/ 10], Step: [ 600/ 600], Loss: 0.8141\n",
            "Epoch: [ 8/ 10], Step: [ 100/ 600], Loss: 0.8359\n",
            "Epoch: [ 8/ 10], Step: [ 200/ 600], Loss: 0.7801\n",
            "Epoch: [ 8/ 10], Step: [ 300/ 600], Loss: 0.8334\n",
            "Epoch: [ 8/ 10], Step: [ 400/ 600], Loss: 0.9358\n",
            "Epoch: [ 8/ 10], Step: [ 500/ 600], Loss: 0.8461\n",
            "Epoch: [ 8/ 10], Step: [ 600/ 600], Loss: 0.7992\n",
            "Epoch: [ 9/ 10], Step: [ 100/ 600], Loss: 0.9098\n",
            "Epoch: [ 9/ 10], Step: [ 200/ 600], Loss: 0.8664\n",
            "Epoch: [ 9/ 10], Step: [ 300/ 600], Loss: 0.7725\n",
            "Epoch: [ 9/ 10], Step: [ 400/ 600], Loss: 0.7397\n",
            "Epoch: [ 9/ 10], Step: [ 500/ 600], Loss: 0.7742\n",
            "Epoch: [ 9/ 10], Step: [ 600/ 600], Loss: 0.7582\n",
            "Epoch: [ 10/ 10], Step: [ 100/ 600], Loss: 0.9384\n",
            "Epoch: [ 10/ 10], Step: [ 200/ 600], Loss: 0.8252\n",
            "Epoch: [ 10/ 10], Step: [ 300/ 600], Loss: 0.6257\n",
            "Epoch: [ 10/ 10], Step: [ 400/ 600], Loss: 0.6863\n",
            "Epoch: [ 10/ 10], Step: [ 500/ 600], Loss: 0.7070\n",
            "Epoch: [ 10/ 10], Step: [ 600/ 600], Loss: 0.7442\n",
            "Accuracy of the model on the 10000 test images:  85 %\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import easydict\n",
        "\n",
        "# argument parser\n",
        "\n",
        "args = easydict.EasyDict({\n",
        "\"batch_size\": 100,\n",
        "\"epochs\": 10,\n",
        "\"lr\": 0.001,\n",
        "})\n",
        "\n",
        "# Hyper Parameters\n",
        "input_size = 784\n",
        "num_classes = 10\n",
        "num_epochs = args.epochs\n",
        "batch_size = args.batch_size\n",
        "learning_rate = args.lr\n",
        "\n",
        "# MNIST Dataset (Images and Labels)\n",
        "train_dataset = dsets.MNIST(root ='./data',\n",
        "        train = True,\n",
        "        transform = transforms.ToTensor(),\n",
        "        download = True)\n",
        "\n",
        "test_dataset = dsets.MNIST(root ='./data',\n",
        "        train = False,\n",
        "        transform = transforms.ToTensor())\n",
        "\n",
        "# Dataset Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "        batch_size = batch_size,\n",
        "        shuffle = True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "        batch_size = batch_size,\n",
        "        shuffle = False)\n",
        "\n",
        "# Model\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "model = LogisticRegression(input_size, num_classes)\n",
        "\n",
        "# Loss and Optimizer\n",
        "# Softmax is internally computed.\n",
        "# Set parameters to be updated.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "\n",
        "# Training the Model\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = Variable(images.view(-1, 28 * 28))\n",
        "        labels = Variable(labels)\n",
        "\n",
        "        # Forward + Backward + Optimize\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # (1)\n",
        "        loss.backward()\n",
        "        # (2)\n",
        "        optimizer.step()\n",
        "        # (3)\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n",
        "                    % (epoch + 1, num_epochs, i + 1,\n",
        "                       len(train_dataset) // batch_size, loss.data.item()))\n",
        "\n",
        "# Test the Model\n",
        "correct = 0\n",
        "total = 0\n",
        "for images, labels in test_loader:\n",
        "    images = Variable(images.view(-1, 28 * 28))\n",
        "    outputs = model(images)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum()\n",
        "\n",
        "print('Accuracy of the model on the 10000 test images: % d %%' % (100 * correct / total))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Skeleton code for Recommendation system"
      ],
      "metadata": {
        "id": "hrMjd2YUiaZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U deepctr-torch\n",
        "!git clone https://github.com/shenweichen/DeepCTR-Torch.git"
      ],
      "metadata": {
        "id": "GQs7hvxb337S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/DeepCTR-Torch/examples\n",
        "!python run_classification_criteo.py"
      ],
      "metadata": {
        "id": "VQcU9-cSkAF0",
        "outputId": "075a0841-d15a-452c-af98-777b9972b8c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DeepCTR-Torch/examples\n",
            "cuda ready...\n",
            "cuda:0\n",
            "Train on 128 samples, validate on 32 samples, 4 steps per epoch\n",
            "Epoch 1/10\n",
            "0s - loss:  0.6420 - binary_crossentropy:  0.6420 - auc:  0.6256 - val_binary_crossentropy:  0.5643 - val_auc:  0.4000\n",
            "Epoch 2/10\n",
            "0s - loss:  0.5083 - binary_crossentropy:  0.5083 - auc:  0.9731 - val_binary_crossentropy:  0.5526 - val_auc:  0.4229\n",
            "Epoch 3/10\n",
            "0s - loss:  0.3937 - binary_crossentropy:  0.3937 - auc:  0.9937 - val_binary_crossentropy:  0.5710 - val_auc:  0.4114\n",
            "Epoch 4/10\n",
            "0s - loss:  0.2521 - binary_crossentropy:  0.2521 - auc:  1.0000 - val_binary_crossentropy:  0.5980 - val_auc:  0.4686\n",
            "Epoch 5/10\n",
            "0s - loss:  0.1425 - binary_crossentropy:  0.1425 - auc:  1.0000 - val_binary_crossentropy:  0.6580 - val_auc:  0.4629\n",
            "Epoch 6/10\n",
            "0s - loss:  0.0905 - binary_crossentropy:  0.0905 - auc:  1.0000 - val_binary_crossentropy:  0.6514 - val_auc:  0.4971\n",
            "Epoch 7/10\n",
            "0s - loss:  0.0629 - binary_crossentropy:  0.0629 - auc:  1.0000 - val_binary_crossentropy:  0.6848 - val_auc:  0.5143\n",
            "Epoch 8/10\n",
            "0s - loss:  0.0453 - binary_crossentropy:  0.0453 - auc:  1.0000 - val_binary_crossentropy:  0.7132 - val_auc:  0.5371\n",
            "Epoch 9/10\n",
            "0s - loss:  0.0336 - binary_crossentropy:  0.0336 - auc:  1.0000 - val_binary_crossentropy:  0.7499 - val_auc:  0.5200\n",
            "Epoch 10/10\n",
            "0s - loss:  0.0267 - binary_crossentropy:  0.0267 - auc:  1.0000 - val_binary_crossentropy:  0.7668 - val_auc:  0.5314\n",
            "\n",
            "test LogLoss 0.5951\n",
            "test AUC 0.6093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://jeageun@bitbucket.org/jeageun/mlcodesign.git\n",
        "!tar zxf mlcodesign/rm_dataset.tgz\n",
        "%cd /content/rm_dataset\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tvz-j4N2ipXU",
        "outputId": "11148c9b-90dc-412e-d05f-c31dec91518e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'mlcodesign'...\n",
            "Unpacking objects: 100% (6/6), done.\n",
            "/content/rm_dataset\n",
            "test1.txt  test2.txt  test3.txt  train1.txt  train2.txt\n"
          ]
        }
      ]
    }
  ]
}